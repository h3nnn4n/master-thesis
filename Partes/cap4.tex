\chapter{Experiments, Results and Analysis}\label{chap:experiments_and_results}

% Present the chapter
In this Chapter the experiment design is presented. Results are shown and an in
depth analysis is conducted, including: Energy and distance metrics for
measuring the performance of the proposed methods; A rigorous statistic testing
of the proposed methods; A comparison to competing methods in the literature in
order to validate the proposed methods; Finally, a visual inspection of the
best predictions and a comparison against the native conformation.
\textcolor{red}{TODO: tem q atualizar aqui conforme for escrevendo}

\section{Design of Experiments}\label{sec:design_of_experiments}

% Present the hardware setup
The experiments were all conducted on a single machine using the same hardware
throughout the full experimentation. Table~\ref{tab:machine-setup}
presents the machine utilized to run all the experiments. Each run of a
prediction method consists of a serial program that run continually without
interruption. The experiments were run in parallel, limited to at most one
running test per core\footnote{Only physical cores were considered. No virtual
(Hyperthreading) core was involved in the computations.}. To ensure maximum
repeatability the machine had no graphical interface enabled or any other form
of user interaction during the course of the experimentation.

\begin{table}[th]
    \centering
    \begin{tabular}{r|l} \hline \hline
        Name & Value \\ \hline \hline
        Operating System & Arch Linux \\ \hline
        Kernel &  Arch Linux Kernel 4.18.16 \\ \hline
        CPU & Intel(R) Core(TM) i5-3570K CPU @ 4.20GHz \\ \hline
        Number of Cores & 4 Physical cores, no hyperthreading cores \\ \hline
        RAM & 16 GB @ 1400 MHz \\ \hline \hline
    \end{tabular}
    \caption{The Machine Setup}
    \label{tab:machine-setup}
\end{table}

The experimentation consisted of running the two proposed methods, namely
SADE-REMC-FINAL and SADE-MC-FINAL. Two of the methods proposed
in~\cite{silva2019self} are also included, Namely SADE-REMC and SADE-MC.
Lastly, the Rosetta Ab Initio protocol is also included, amounting to five
different methods being ran.

The analysis is divided in two steps. In the first one, the two proposed methods
are compared agaisnt the works in~\cite{silva2019self} and the Rosetta Ab Initio
Protocol.
% Present the "in house" comparison
The metrics utilized in this are the \textit{scorefxn} energy value of the
best solution and the \ac{RMSD} associated with the same conformation. The
results were collected over 50 independent runs of each method for each target
protein. A graphical analysis is conducted in order to identify visually the
relative performance between the proposed methods. Considering that a visual
analysis is not enough (in this case), a more rigorous numerical statistic set
of test is conducted.  The Shapiro-Wilk~\cite{wilk1968joint} normality test is
employed with a confidence level of 5\%, i.e. $\alpha = 0.05$, to assess the
presence (or lack) of a underlying normal distribution. Based on its result, a
parametric/non-parametric test is employed with a confidence level of $\alpha =
0.05$. Due to the multiple comparisons involved \v{S}idák's $\alpha$ correction
will be utilized~\cite{vsidak1967rectangular}. The winner (or winners)
method(s) will be used in further comparison against the literature. The
processing time for this stage is also analyzed. The time required from the
start of the initial population generation until the final full atom model
output is measured in seconds.
\textcolor{red}{TODO: colocar parameter control analysis}
\textcolor{red}{TODO: convergence analysis}
\textcolor{red}{TODO: ffi analysis}
\textcolor{red}{TODO: mc vs remc analysis}

% Explain the clustering results
The use of clustering to extract and return different conformations from the
proposed methods require extra steps during the analysis. First, the main use of
returning several conformations is to allow an human expert to choose one that
has the desired properties. This, can not be automated in a test enviroment where
hundres of experiments are performed computationaly each day. As such, the human
expert, for the purposes of performance evaluation, must be replaced by a
computer oracle. This oracle can always find the conformation with the lowest
RMSD or the conformation with the lowest energy. This, of course, would not
be possible in a real world scenario where a protein without a know structure is
being predicted. With that in mind, the analysis of the two proposed methods is
devided in two sub groups. The first is named \texttt{best-by-rmsd} and the second
is named \texttt{best-by-energy}.

% Present the "free for all" analysis
In the second analysis step, a direct comparison against several works in the
literature is considered. Since there is a severe lack of standardization in the
literature regarding experimentation, the following methodology was used. Works
that provided the best RMSD had their proteins listed. The proteins that occured
the most were used for comparison. It is worth noting that the majority of works
provide little information about how the experimentation was conducted. The way
in which the results are analysed lacks a standard. As such, this work does a
direct comparison using the best RMSD achieved in a set of runs. While this is
not ideal, due to different works running each method different number of times,
this is possibly the only way to a comparison against several works. The presence
of outliers also weaken this comparison. Also, since only the most used proteins
are select, it is possible that some works are only represented partially, i.e.
some of the results are left out. Nevertheless, at the end of the day for the
PSPP what matters is having the lowest possible error. As such, comparing just
the best RMSD still a worthwhile analysis, albeit not ideal.

% Present the protein set
With that in mind, the set of proteins presented in Table~\ref{tab:protein-targets}
was chosen. The column \textbf{Name} contains the protein identification code
as in PDB.  The \textbf{Size} column shows the number of amino acids in the protein.
The \textbf{Backbone Angles} column shows the number of angles in the backbone,
this also has a one to one relation to the number of variables to be optimized
for a given protein. The \textbf{Structure} column holds the secondary
structures present in the protein set represented by $\alpha$-helices or
$\beta$-sheets.

\begin{table}[bh]
  \centering
  \begin{tabular}{ l | c | c | c | c }
    \hline \hline
    Name & Size & Backbone Angles & Structure         \\ \hline \hline
    1L2Y & 20   & 60              & $2\alpha$         \\ \hline
    1WQC & 26   & 78              & $2\alpha$         \\ \hline
    1ACW & 29   & 87              & $1\alpha, 2\beta$ \\ \hline
    1ZDD & 35   & 105             & $2\alpha$         \\ \hline
    2MR9 & 44   & 132             & $3\alpha$         \\ \hline
    1CRN & 46   & 138             & $2\alpha, 2\beta$ \\ \hline
    1ENH & 54   & 162             & $3\alpha$         \\ \hline
    1ROP & 63   & 189             & $2\alpha$         \\ \hline
    1UTG & 70   & 210             & $4\alpha$         \\ \hline
    1AIL & 72   & 216             & $3\alpha$         \\ \hline
    \hline
  \end{tabular}
  \caption{Target proteins and their features}
  \label{tab:protein-targets}
\end{table}

% present each protein (or maybe not)

% present the two algorithms (MC vs REMC)

% Present the parameters
The two proposed methods all operates with the same parameters, as presented
in~\ref{tab:parameters}.The first columns contains the parameter name and the
second one its respective value. The SaDE learning Phase has its default value,
as presented in~\cite{qin2009differential}. There are 100 simultaneous
trajectories throughout the execution, i.e. a population size of 100. A million
function evaluations are available for the optimization phase, where each
fragment insertion routine can use up to 25 at a time. \ac{FFI} uses a fragment
size of 9 and is applied with a probability of 2\% before each standard fragment
insertion. The other methods being compared uses the same values from
Table~\ref{tab:parameters} as apliable.

\begin{table}[ht]
    \centering
    \begin{tabular}{r|l} \hline \hline
        Parameter & Value \\ \hline \hline
        SaDE learning Phase & 50 \\ \hline
        Population Size & 100 \\ \hline
        Function Evaluation Budget & 1000000 \\ \hline
        MC/REMC Function Evaluation Budget & 25 \\ \hline
        Spicker cluster size & 10 \\ \hline
        Crowding factor & 5 \\ \hline
        Crowding distance method & RMSD \\ \hline
        \ac{FFI} probability & 0.02 \\ \hline
        \ac{FFI} length & 9 \\ \hline \hline
    \end{tabular}
    \caption{Parameters utilized in the proposed methods}
    \label{tab:parameters}
\end{table}

\section{Energy and \ac{RMSD} Analysis}\label{sec:methods-analysis}

% Present the goals
In this section the results from the experiments are analysed. The analysis
is devides in two subsections. In Subsection~\ref{sec:statistical-analysis} the
results are display and a sequence of statistical analysis is presented.
Subsection~\ref{sec:pareto-front-analysis} presents an analysis of the algorithms
using a pareto front analysis. Subection~\ref{sec:analysis-conclusions} presents
the conclusions from these analysis.

\subsection{Statistical Analysis}\label{sec:statistical-analysis}

% Introduce the analysis
Due the amount of data generated by the experiments, most of the analysis will
be conducted by visual inspection or by summarizing the results and keypoints.
However, for the sake of completude and scientific rigor, complementary
information is provided in the Appendices.

% Present the box plots
In Figure~\ref{fig:boxplot-rmsd} the RMSD from the predictions is presented.
The proteins, presented in the x axis, are displayed in lexicographical order.
The y axis presents the RMSD, where lower is better. The methods are grouped
horizontally by protein.

It is possible to see that for 9 of the 10 proteins the
two proposed methods had a very close overall performance, considering the median.
However, for protein 1rop, sade-mc-final outperformed the other methods by a
significant margin. Moreover, the two proposed methods had better medians than
the other methods (not including rosetta) in 8 of the 10 proteins. For 1rop
sade-mc-ffi had the second best result, behind sade-mc-final, while sade-remc-final
had the second worst result. For 1zdd, the two proposed methods had the two worst
medians, however, by a relativelly small amount.

In a direct comparison agaisnt rosetta, in proteins 1acw, 1enh, 1l2y, 1utg and 2mr9
the proposed methods had a significant improved upon rosetta. For 1crn the rosetta
appears to have outperformed the proposed methods. In the remaining proteins visual
inspection is not enough to acuratelly detect significant performance differences.

Some other behaviours are worth noting too. The manisfestation of uniqueness each
proteins is visible in the distribution of different methods. For example, on
2mr9, rosetta had a worst performance than the other methods by a significant margin,
whereas the other methods had a versy similar performance. For 1utg, the two proposed
methods outclassed the other methods, including rosetta. Forthermore, the worst
result from sade-remc-final was better than the medians of all the other methods.
Now, interestingly, for 1wqc and 1crn, rosetta and the two proposed methods had
better performance than the other methods. Also, for some proteins, such as 1enh,
the overall performance is much closer. One possible explanation for this difference
in results, considering that all the methods had the same input, is the conformation
sampling strategy. It might best suit some proteins with a given conformation
structure more than others.

\begin{figure}
  \includegraphics[width=\linewidth]{Figuras/boxplots/boxplot_best_by_rmsd_rmsd_after.pdf}
  \caption{Boxplot presenting the RMSD for the protein predictions with the
    competing methods. For the two proposed methods, the \texttt{best-by-rmsd}
    data was used}
  \label{fig:boxplot-rmsd}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{Figuras/boxplots/boxplot_best_by_energy_scorefxn.pdf}
  \caption{Boxplot presenting the \texttt{scorefxn} for the protein predictions with the
    competing methods. For the two proposed methods, the \texttt{best-by-energy}
    data was used}
  \label{fig:boxplot-energy}
\end{figure}

Figure~\ref{fig:boxplot-energy} presents data similarly to the previous figure,
however, the y axis now represents the \texttt{scorefxn} energy function. Considering
the energy results, when compared to the rmsd boxplot, the results are relativelly
more similar. Nevertheless, for some proteins there are some behaviours that are
more visible. For 1rop, sade-remc-final appears to have had a worst performance
than the other methods. Same as with the RMSD data. For 1crn, rosetta and the two
proposed methods appears to have a better result overall. For 2mr9, rosetta
appears to be lagging behind in performance. These observations are just visual
trends which helps understand the relative performance. In order to properlly
access their differences a more rigorous statistical approach is required.

% Present shapiro-wilk RESULTS
To scientifically assess the performance of the proposed methods relative to
other competing methods, proper statistical tests are required. A pre-requisite
before applying the tests which evaluate performance, is to detect the
underlying distribution of the data to be analysed. There are several ways to
acomplish this. One, is to visualy inspect a histogram or a Quartile-Quantile
plot. A more rigorous approach is to use a statistical test which can evaluate
the presence of a gaussian distribution. One such test is Shapiro-Wilk and it
will be utilized for this purpose.

For the tests an $\alpha = 0.05$ was utilized. The RMSD and \texttt{scorefxn}
of both the \texttt{best-by-rmsd} and \texttt{best-by-energy} were analysed for
all the methods. For the sake of breviety the results are exposed outside this
text, in Appendix~\ref{appendix:shapiro}. The null hyphotesis, $H_0$, is that
the samples belong to a normal distribution. Therefore, if the test returns a $p$
value less than $\alpha$, $H_0$ is rejected and the underlying distribution is
not gaussian. If $p$ is equal or bigger than $\alpha$, then the test failed to
reject $H_0$. Considering the data to be analyzed, this is not so straight
forward. There are 2 metrics to be analysed, the RMSD and \texttt{scorefxn}.
There are several methods which were executed. Furthermore, several proteins
were utilized for testing. Not only this gives a bug amount of data to analyse,
it is possible that some method, protein or metric may lead to a gaussian
distribution, where others don't.

For the RMSD using \texttt{best-by-rmsd}, there were $37$ cases where $H_0$
failed to be rejected, and $23$ where it was rejected. All methods when applied
to protein 1acw gave a gaussian distribution. For 1zdd, 5 of the 6 methods gave
a non-gaussian distribution. The proteins, 1crn, 1l2y and 2mr9, had split results
where half the methods rejected $H_0$ and half didn't. Some proteins appear to
be more propense to generate gaussian distributions than others. By analysing
the methods instead of the proteins, a new insight is given. The two proposed
methods had $2$ and $1$ rejection of $H_0$, for sade-mc-final and sade-remc-final,
respectively. Whereas sade-mc-ffi and sade-remc both had $6$ rejections. This
indicates that some methods are more propense to generate gaussian distribution
than others.

Considering \texttt{scorefxn} with \texttt{best-by-energy}, there were $46$
cases whenre $H_0$ failed to be rejected, and $14$ were it was rejected. On this
situation, with the given metric, more methods are generating gaussian
distributions. Three proteins, 1acw, 1crn and 1l2y, failed all times to reject $H_0$.
For 1ail and 1enh, the results were split in the middle. 1zdd had the most
rejections of $H_0$, with $4$. The remaining proteins had only a single
rejection of $H_0$. with both metrics, 1zdd had the most rejections of $H_0$,
whereas all methods on 1acw failed to reject $H_0$. Looking at the methods,
there is no clear trend. The proposed method sade-remc-final had no rejection
of $H_0$, while sade-mc-final had two.

% Conclude the analysis and introduce the next analysis
Given this, it appears that the underlying distribution, in most cases, is
gaussian. However, there are consistent, albeit fewer, situations where they are
not gaussian. As such, a non-parametric test must be employed. In this case, the
analysis is split in two step. Firstly, Kruskal–Wallis test is applied in order
to detect if there is a significant difference between the distribution, i.e.
if there is a method that is better than some other. If there is, then
a pairwise Mann-Whitney is utilized. Even though it is a non-parametric test, it
has, relative to Student's t test, a similar performance. Therefore,
Mann-Whitney's test can suit the needs to analyse the data at hand. Running
Mann-Whitney only if Kruskal–Wallis detects a significant difference is
important, as it helps to prevent inference errors.

% Present kruskal-wallis RESULTS
The Kruskal–Wallis test was performed with an $\alpha = 0.05$. The null-hypothesis
is that all the samples were drawn from the same distribution, i.e. all the
methods have equivalent performance.
Table~\ref{tab:kruskal-wallis-best-by-rmsd-RMSD} presents the result of the
tests for the RMSD.
The column protein indicates the name of the protein, and the column
$p$-value indicates the result of the test. When the $p$-value is less than
$\alpha$ it is hilighted in boldface. The result if very straightforward to
interpret. All the proteins with the exception of 1zdd had a $p$-value less
than the selected $\alpha$. Moreover, when $H_0$ was rejected, the $p$-value
was less than $1e-05$ in all cases. For 1zdd, $H_0$ failed to be rejected, the
most likely reason is that since this protein is (arguably) the easiest one,
all methods reached near-native conformations.

Table~\ref{tab:kruskal-wallis-best-by-energy-scorefxn} reads the same
as the previous one, however, the data it displays regards to \texttt{scorefxn}.
Protein 1zdd failed to reject $H_0$ again, as did 1l2y, another relatively
simple and short protein. For all the remaining proteins $H_0$ was rejected.
Therefore, it is safe to do a pair-wise test using Mann-Whitney's.

\input{Tabelas/kruskal.tex}

% Present mann-whitney RESULTS
The Mann-Whitney test was applied with $\alpha = 0.05$. The null hypothesis is
that the two distributions are equal, i.e. both methods have the same
performance. Rejection of $H_0$ indicates that one method is better than other.
Considering that both RMSD and \texttt{scorefxn} will be analized, each for
10 proteins, a total of $20$ tables are necessary to expose all the data. As
such, this information is exposed in Appendix~\ref{appendix:mann-whitney-rmsd}
and~\ref{appendix:mann-whitney-scorefxn}, where the first presents the data for
RMSD and the second for the energy. The resuls will be summarized reporting the
overall results from the test. To keep focus and simplicity, first, the two
proposed methods will be compared agaisnt the other methods. Then, they will be
compared against rosetta. In both cases, the RMSD and \texttt{scorefxn} will be
considered for the analysis.

\input{Tabelas/mann-whitney-summary-internal.tex}

Starting with the RMSD using \texttt{best-by-rmsd}, comparing the two proposed
methods against the ones from a previous work, the results are shown in
Table~\ref{tab:mann-whitney-summary-internal-best-by-rmsd-RMSD}. The column
Protein displays the protein name. Column Wins, Loses and Draws shows the number
of times that any of the two proposed methods outperformed, were outperformed,
or had a similar performance than any of the other methods (excluding rosetta).
Based on the results from Kruskal–Wallis, as show in
Tables~\ref{tab:kruskal-wallis-best-by-rmsd-RMSD} and
Tables~\ref{tab:kruskal-wallis-best-by-energy-scorefxn}, there is not
difference between the methods for protein 1zdd both when considering RMSD and
\texttt{scorefxn}. For 1l2y also there is no difference, however, only with
\texttt{scorefxn}. As such, these rows in the table should be ignored.

For all but two proteins, the two proposed methods outperformed the others. For
1crn there is one of the proposed method that was outperformed. In this case,
sade-mc-final was outperformed by sade-remc-final. That is, one of the proposed
methods outperformed the other. For 1rop, sade-mc-final outperformed all other
methods, including sade-remc-final. Furthermore, it was outperformed by all
methods but sade-remc-ffi. For all proteins, excluding 1zdd, the proposed
methods outperformed the other methods more times than they were outperformed.

The summary concerning the energy of the conformations is show in
Table~\ref{tab:mann-whitney-summary-internal-best-by-energy-scorefxn}. It
reads the same as the table before. For proteins 1ail, 1crn, 1enh, 1wqc and 2mr9
the proposed methods outperformed the others more times than they were
outperformed. On 1acw, sade-mc-final outperformed sade-remc-final, while
sade-remc-final was outperformed by the other 3 methods. The exact situation
occured for 1rop. On 2mr9, sade-mc-final outperformed sade-remc-final, while
sade-mc-final was outperformed by 2 other methods. Interestingly, on 1zdd
and 1lwy, Mann-Whitney detected performance differences, disagreeing with
the results from Kruskal–Wallis. Nevertheless, for these 2 proteins, the result
from Kruskal–Wallis will be considered to be the correct one.

\input{Tabelas/mann-whitney-summary-rosetta.tex}

In Table~\ref{tab:mann-whitney-summary-rosetta-best-by-rmsd-RMSD} the
comparison of the two proposed methods compared to Rosetta using RMSD is
presented. For 1acw, 1enh, 1l2y, 1utg and 2mr9, totalling 5 proteins, both
proposed methods outperformed rosetta. For 1ail only sade-remc-final
outperformed rosetta. On 1rop, sade-mc-final outperformed rosetta, while
sade-remc-final was outperformed. On 1crn, only sade-mc-final was outperformed
by rosetta. From this, it is demonstrated that both the proposed methods
consistenly outperform rosetta. In fact, rosetta outperformed the proposed
methods only in 2 ocasions.

The comaparison of rosetta and the proposed methods using \texttt{scorefxn}
is presented in Table~\ref{tab:mann-whitney-summary-rosetta-best-by-rmsd-RMSD}.
For this particular case, for 3 proteins, 1enh, 1utg and 2mr9, both proposed
methods outperformed rosetta. On 1acw and 1rop rosetta outperformed only
sade-remc-final. On 1ail, only sade-mc-final outperformed rosetta. Overall,
the proposed methods outperformed rosetta in some cases and had equal
significance in others.

In light of these observations, it is safe to consider that the two proposed
methods outperform the previous methods. It also outperforms rosetta, one of
the state of the art methods in the literature and CASP winners.

\subsection{Pareto Front Analysis}\label{sec:pareto-front-analysis}

\textcolor{red}{Muitos dados pra analisar. Pouco retorno. Vou deixar de lado}

% \subsection{Analysis Conclusions}\label{sec:analysis-conclusions}

% Analyze the overall results and conclude

\section{Convergence and Diversity Analysis}
% AVG rmsd plot

\section{Parameter Analysis and Operator Usage}
% Most used parameters over time

\section{Forced Fragment Insertion Analysis}
% What was the impact? How good/bad it was

\section{Repacking Impact}
% How much does the rmsd change before after
% If ranked by energy, does the ranking change before/after repacking

\section{Monte Carlo vs Replica Exchange Monte Carlo}
% Compare sade-mc-final and sade-remc-final

\section{Processing time}

\section{Comparison with Competing Methods}
% Big table here

A comparison against methods in the literature is very difficult to conduct.
Most of the methods in the literature are relativelly superficial in their
explanations of how a given algorithm was implemented and what testing
methodology was employed. Paper space seems to be a possible cause for this,
since articles that spans more pages are usually more detailed about the
implementation and methodology. As such, a comparison has to be based on the
data provided in the works, which in most cases is not enough for a proper
rigorous analysis to be conducted. Nevertheless, this work attempts to provide
a simple framework for comparing the proposed methods with works in the
literature. Several works were selected, where the model utilized was the
full atomic model with and ab initio method, and their proteins and the RMSD
of the best predition was recorded. From these, the proteins which appeared in
more than four works were selected.

\input{Tabelas/literature_comparison.tex}

In Table~\ref{tab:literature-comparison}, this data is presented. The first
column indicates the year of publication, where the data is presented in
decreasing order. The column Source presents the source of the data, which is
either a work in the literature, one of the proposed methods, or the results
from rosetta that were collected during the development of this work. The
remaining columns present several proteins, sorted by the frequency in which it
appears in literature. The data in these columns is the best RMSD from the
method in the given work.

Given that more than 20 works are cited in this table, doing a detailed
analysis and description would be rather cumbersome. Instead, some keynotes are
provided. First, there appears to be a weak trend where the RMSD is slowly
decreasing. Albeit this trend this trend has several counter examples.
For instance, the works of~\cite{cazacu2014steel} and~\cite{judy2009multi}
have competitive results even by today standards, even though they are more than
a decade old. Another major point, is that the results are getting extremaly
competitive in the last years. The competition probably is not any bigger due to
the lack of common proteins between the workers. It is not rare to find works
from the same author where a different set of proteins is utilized.

Even in face of the competitiveness of the problem, the proposed methods were
able to achieve the best RMSD in a couple of proteins. In many other proteins
the results were competitive with the state of the art. For the protein 1wqc
the best RMSD was achieved by sade-mc-final. On 2mr9 the proposed methods
achieved the best and second best results. For 1rop and 1crn, the two most used
proteins in the literature, one of the proposed methods was able to achieve
the second best result. The same occured for 1enh. Another point worth stating
is that some proteins might be way too easy for the current methods. Take 1zdd
for example, all but two methods had RMSD smaller than 2.62. Considering that
most proteins in PDB have a resolution ranging from 1 to 2\AA, trying to go
smaller than that is more a pursue of luck than science. As such, this protein
might only be useful to validating new methods, but not for measuring progress.

\section{GDT-TS and TM-Score metrics}

\section{Visual Representation of the Predictions}

A final step in the analysis of the proposed methods is to do a visual
inspection. Considering that two methods were proposed, and that there are 10
proteins being analysed, comparing the 20 best predictions would be very
extensive. For this reason, sade-mc-final was selected to be inspected, since
its preditions were over all more accurate.

\input{Figuras/conformations.tex}

In Figure~\ref{fig:all-conformations} all conformations from the 10 best
predictions by sade-mc-final are presented. The proteins are presented in
lexicographical order. The predicted conformation is presented in green and the
native conformation is presented in red\footnote{For the readers with a black
and white copy, the predicted conformation is in a light shade of grey, while
the native conformation is in dark grey.}.

Proteins 1enh, 1rop, 1utg, 1wqc, 1zdd and 2mr9 had near native conformations.
There is very little to comment for these particular predictions. For protein 1acw,
which had a relativelly high RMSD, consideringthe size of the protein, there are
two main prediction errors. Firstly, the $\beta$-sheets did not form, and, where
should be one, there is an $\alpha$ helix instead. The second error, is that the
$\alpha$ helix is in the wrong place and split. It starts where it should,
however, it only has a single turn. A second helix forms at the eighth residue
and goes on for ten more residues. Both these erros can be traced down to an
error in the Secondary Structure Prediction, which the proposed methods have no
way to avoid. For 1ail, the prediction is mostly correct, however, two helixes
are split apart. The first, to the left of the image, and the second, in the
middle helix. These errors were prediction failures that occured in the proposed.
This protein in particular has a correct secondary structure as an starting
point.

On 1crn, a relatively complex protein, has a overall correct fold. The fine
details, however, are lacking. The two helixes are mostly missing and the
sheets did not fold. These two errors can be traced down to the secondary
structure prediction used as input. For 1l2y a similar scenario occurs, where
the main source of error is in the data fed to the prediction engine. The
overall shape of the protein was correctly predicted. The helix is completly
missing.

Interestingly, on 4 of the proteins with the biggest visually detectable error,
3 of them had its source of error outside to the prediction. The proposed method
relies heavilly on the predicted secondary structure, and as such, has do deal
with uncentanty. In only a single case the major error in a prediction was
generated during the prediction it self, as occured on 1ail.
